{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import time\n",
    "import cv2 as cv2\n",
    "import numpy as np\n",
    "from imgaug import augmenters as ia\n",
    "from matplotlib.image import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from IPython import display\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.contrib.framework.python.ops import variables\n",
    "from tensorflow.contrib.layers.python.layers import utils\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.ops import nn\n",
    "DATA_FORMAT_NCHW = 'NCHW'\n",
    "DATA_FORMAT_NHWC = 'NHWC'\n",
    "height = width = 512 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAugmentation:\n",
    "    def __init__(self):\n",
    "        print('DataAugmentation')\n",
    "    \n",
    "        \n",
    "    def augment(self, images):\n",
    "        rand_val=random.randint(0,360)\n",
    "        rand_scale = random.uniform(0.9,1.1)\n",
    "        images = ia.Affine(rotate=rand_val,cval=(128, 128)).augment_image(images) # randomly rotates between 0 to 360 ??180??\n",
    "        images = ia.Fliplr(1.0).augment_image(images) # horizontally flip all of the images\n",
    "        images = ia.Scale(rand_scale).augment_image(images) # scales all images to 90 percent of their original size.\n",
    "        new_size = images.shape[0]\n",
    "        #print('new_size:',new_size)\n",
    "        if new_size<=512:\n",
    "            delta = width - new_size\n",
    "            top, bottom = delta//2, delta-(delta//2)\n",
    "            left, right = delta//2, delta-(delta//2)\n",
    "            color = [0, 0, 0]\n",
    "            images = cv2.copyMakeBorder(images, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n",
    "        else:\n",
    "            delta = (new_size - width)//2\n",
    "            if new_size%2==0:\n",
    "                end = new_size-delta\n",
    "            else:\n",
    "                end = new_size-delta-1\n",
    "            images = images[delta:end,delta:end]\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_softmax(features,\n",
    "                    temperature=None,\n",
    "                    name=None,\n",
    "                    variables_collections=None,\n",
    "                    trainable=True,\n",
    "                    data_format='NHWC'):\n",
    "    \"\"\"Computes the spatial softmax of a convolutional feature map.\n",
    "    First computes the softmax over the spatial extent of each channel of a\n",
    "    convolutional feature map. \n",
    "   \"Learning visual feature spaces for robotic manipulation with\n",
    "   deep spatial autoencoders.\" Finn et al., http://arxiv.org/abs/1509.06113.\n",
    "   Args:\n",
    "    features: A `Tensor` of size [batch_size, W, H, num_channels]; the\n",
    "      convolutional feature map.\n",
    "    temperature: Softmax temperature (optional). If None, a learnable\n",
    "      temperature is created.\n",
    "    name: A name for this operation (optional).\n",
    "    variables_collections: Collections for the temperature variable.\n",
    "    trainable: If `True` also add variables to the graph collection\n",
    "      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n",
    "    data_format: A string. `NHWC` (default) and `NCHW` are supported.\n",
    "   Returns:\n",
    "    feature_keypoints:Softmax Attention values\n",
    "   Raises:\n",
    "     ValueError: If unexpected data_format specified.\n",
    "     ValueError: If num_channels dimension is unspecified.\n",
    "   \"\"\"\n",
    "    with variable_scope.variable_scope(name, 'spatial_softmax'):\n",
    "        shape = array_ops.shape(features)\n",
    "        static_shape = features.shape\n",
    "        if data_format == DATA_FORMAT_NHWC:\n",
    "            height, width, num_channels = shape[1], shape[2], static_shape[3]\n",
    "        elif data_format == DATA_FORMAT_NCHW:\n",
    "            num_channels, height, width = static_shape[1], shape[2], shape[3]\n",
    "        else:\n",
    "            raise ValueError('data_format has to be either NCHW or NHWC.')\n",
    "        if num_channels.value is None:\n",
    "            raise ValueError('The num_channels dimension of the inputs to '\n",
    "                       '`spatial_softmax` should be defined. Found `None`.')\n",
    "\n",
    "    with ops.name_scope('spatial_softmax_op', 'spatial_softmax_op', [features]):\n",
    "        pos_x, pos_y = array_ops.meshgrid(\n",
    "          math_ops.lin_space(-1., 1., num=height),\n",
    "          math_ops.lin_space(-1., 1., num=width),\n",
    "          indexing='ij')\n",
    "        pos_x = array_ops.reshape(pos_x, [height * width])\n",
    "        pos_y = array_ops.reshape(pos_y, [height * width])\n",
    "\n",
    "        if temperature is None:\n",
    "            temp_initializer = init_ops.ones_initializer()\n",
    "        else:\n",
    "            temp_initializer = init_ops.constant_initializer(temperature)\n",
    "        if not trainable:\n",
    "            temp_collections = None\n",
    "        else:\n",
    "            temp_collections = utils.get_variable_collections(\n",
    "            variables_collections, 'temperature')\n",
    "\n",
    "        temperature = variables.model_variable(\n",
    "          'temperature',\n",
    "          shape=(),\n",
    "          dtype=dtypes.float32,\n",
    "          initializer=temp_initializer,\n",
    "          collections=temp_collections,\n",
    "          trainable=trainable)\n",
    "        features = array_ops.reshape(features, [-1, num_channels ,196]) \n",
    "        softmax_attention = nn.softmax(features / temperature)\n",
    "        softmax_attention = array_ops.reshape(softmax_attention, [-1, 14,14,num_channels])      \n",
    "    return softmax_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Anet:\n",
    "    def __init__(self):\n",
    "        print('Anet')\n",
    "        \n",
    "\n",
    "    def model_anet1(self,feature_map, scope): \n",
    "        #Layer 1: Convolutional. Input = 14x14x1024 Output = 5X14x14.  \n",
    "        \n",
    "        mu = 0\n",
    "        sigma = 0.1\n",
    "        \n",
    "        scope_name = scope + str(1)\n",
    "        with tf.variable_scope(scope_name):\n",
    "            conv_W = tf.Variable(tf.truncated_normal(shape=(1, 1, 1536, 5), mean = mu, stddev = sigma))\n",
    "            score_map = tf.nn.conv2d(feature_map, conv_W, strides=[1, 1, 1, 1], padding=\"SAME\") \n",
    "            score_map = tf.nn.softmax(score_map,name=\"score_map\")\n",
    "            print(score_map)\n",
    "        return score_map\n",
    "    \n",
    "    def model_anet2(self,feature_map, scope): \n",
    "        #Layer 1: Convolutional. Input = 14x14x1536. \n",
    "        # changed since now we're passing features for 2 eyes, therefore the size is Input = 14x14x3072\n",
    "        # Todo: concatenation 2 eyes is not required!!!!!!\n",
    "        \n",
    "        mu = 0\n",
    "        sigma = 0.1\n",
    "        epsilon=0.001\n",
    "        \n",
    "        scope_name = scope + str(2)\n",
    "        with tf.variable_scope(scope_name):\n",
    "            conv1_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 1536, 1024), mean = mu, stddev = sigma))\n",
    "            attn_conv1 = tf.nn.conv2d(feature_map, conv1_W, strides=[1, 1, 1, 1], padding=\"SAME\") \n",
    "            params_shape = attn_conv1[-1:]\n",
    "            axis = list(range(len(attn_conv1.get_shape()) - 1))\n",
    "            mean, variance = tf.nn.moments(attn_conv1, axis)\n",
    "          \n",
    "            attn_conv1 = tf.contrib.layers.batch_norm(attn_conv1)\n",
    "            attn_conv1 = tf.nn.relu(attn_conv1)\n",
    "\n",
    "            conv2_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 1024, 1024), mean = mu, stddev = sigma))\n",
    "            attn_conv2 = tf.nn.conv2d(attn_conv1, conv2_W, strides=[1, 1, 1, 1], padding=\"SAME\") \n",
    "          \n",
    "            params_shape = attn_conv2[-1:]\n",
    "            axis = list(range(len(attn_conv2.get_shape()) - 1))\n",
    "            mean, variance = tf.nn.moments(attn_conv2, axis)\n",
    "          \n",
    "            attn_conv2 = tf.contrib.layers.batch_norm(attn_conv2)\n",
    "            attn_conv2 = tf.nn.relu(attn_conv2)\n",
    "\n",
    "            conv3_W = tf.Variable(tf.truncated_normal(shape=(1, 1, 1024, 5), mean = mu, stddev = sigma))\n",
    "            attn_conv3 = tf.nn.conv2d(attn_conv2, conv3_W, strides=[1, 1, 1, 1], padding=\"SAME\") \n",
    "            attn_map = spatial_softmax(features=attn_conv3,name='spatial_softmax')  \n",
    "        return attn_map\n",
    "      \n",
    "    def model(self,feature_map, scope):\n",
    "        anet_endpoint = {}\n",
    "        self.S = self.model_anet1(feature_map, scope)\n",
    "        anet_endpoint['S'] = self.S\n",
    "        self.A = self.model_anet2(feature_map, scope)\n",
    "        anet_endpoint['A'] = self.A\n",
    "        self.G = tf.math.multiply(self.S,self.A,name='gated_maps')\n",
    "        anet_endpoint['G'] = self.G\n",
    "        y = tf.math.reduce_sum(self.G,axis=(1,2),name='predictions')\n",
    "        anet_endpoint['y'] = y\n",
    "        return anet_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/models/research/slim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'CHECKPOINT_DIR' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open 'inception_resnet_v2_2016_08_30.tar.gz'\n",
      "'mv' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "#CHECK_PATH -- Uncomment if required\n",
    "#!CHECKPOINT_DIR=/tmp/checkpoints\n",
    "#!mkdir  ${CHECKPOINT_DIR}\n",
    "#!wget http://download.tensorflow.org/models/inception_resnet_v2_2016_08_30.tar.gz\n",
    "#!tar -xvf inception_resnet_v2_2016_08_30.tar.gz\n",
    "#!mv inception_resnet_v2_2016_08_30.ckpt ${CHECKPOINT_DIR}\n",
    "#!rm inception_resnet_v2_2016_08_30.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'git' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/tensorflow/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-1b0a2e49e3bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minception_resnet_v2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minception_resnet_v2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minception_resnet_v2_base\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minception_resnet_v2_arg_scope\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "from models.research.slim.nets.inception_resnet_v2 import inception_resnet_v2, inception_resnet_v2_base, inception_resnet_v2_arg_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_augmentation = DataAugmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the array of augmented imgs\n",
    "def augment_image(phase,file_list, df_long_labels):\n",
    "    augmented_imgs = []\n",
    "    file_names = []\n",
    "    obj_augmentation = DataAugmentation()\n",
    "    df_augmented_list = pd.DataFrame({\"image\":[]})\n",
    "    for f in file_list: \n",
    "        image = np.load(f)\n",
    "        augmented_image = obj_augmentation.augment(image)\n",
    "        base = os.path.basename(f)\n",
    "        file_name = os.path.splitext(base)[0]\n",
    "        short_name = file_name[:file_name.rfind(\"_\")]\n",
    "        file_names.append(file_names)\n",
    "        df_temp = pd.concat([pd.DataFrame([file_names], columns=['image'])], ignore_index=True)\n",
    "        df_augmented_list = df_augmented_list.append(df_temp)\n",
    "        augmented_imgs.append(augmented_image)    \n",
    "    df_labels = df_augmented_list.merge(df_long_labels)\n",
    "    labels = df_labels[\"level\"].values\n",
    "    #print('labels:',labels)\n",
    "    training_labels = to_categorical(labels,5)\n",
    "    timestamp2 = time.time()    \n",
    "    #print(\"End of Augmentation took %.2f seconds\" % (timestamp2 - timestamp1))\n",
    "    return (training_labels, augmented_imgs,file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "config = {}\n",
    "config['batchsize'] = 4  # can change it, but might become noisy\n",
    "config['learningrate'] = 0.00001 # can change to 0.01\n",
    "config['numEpochs'] = 100 # can change to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anet = Anet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computed_weighted_loss(logits,labels):\n",
    "    class_weights = tf.constant([1, 14, 6, 40, 49],dtype=tf.float32)\n",
    "    class_weights = tf.div(class_weights,tf.reduce_sum(class_weights))\n",
    "    prod =class_weights * labels\n",
    "    weights = tf.reduce_sum(prod,keepdims=True)\n",
    "    # compute your (unweighted) softmax cross entropy loss\n",
    "    unweighted_losses = tf.nn.softmax_cross_entropy_with_logits_v2(labels = labels, logits = logits)\n",
    "    # apply the weights, relying on broadcasting of the multiplication\n",
    "    weighted_losses = unweighted_losses * weights\n",
    "    # reduce the result to get your final loss\n",
    "    loss = tf.reduce_mean(weighted_losses)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define placeholders\n",
    "class_weights = tf.constant([1, 14, 6, 40, 49],dtype=tf.float32)\n",
    "class_weights = tf.div(class_weights,tf.reduce_sum(class_weights))\n",
    "\n",
    "mnet_inputs = {}\n",
    "mnet_inputs['data'] = tf.placeholder(tf.float32, [None, 512, 512, 3], name = \"mnet_data\")\n",
    "mnet_inputs['labels'] = tf.placeholder(tf.float32, [None,5], name = \"mnet_labels\")\n",
    "mnet_inputs['phase'] = tf.placeholder(tf.bool, name = \"mnet_phase\")\n",
    "mnet_inputs['anet'] = tf.placeholder(tf.float32, [None, 14, 14, 1536], name = \"anet_data\")\n",
    "\n",
    "# Define a dictionary for storing curves\n",
    "anet_curves = {}\n",
    "anet_curves['training_loss'] = []\n",
    "anet_curves['validation_loss'] = []\n",
    "anet_curves['training_accuracy'] = []\n",
    "anet_curves['validation_accuracy'] = []\n",
    "\n",
    "anet_endpoint = anet.model(mnet_inputs['anet'], \"Anet\")\n",
    "anet_cross_entropy = computed_weighted_loss(anet_endpoint[\"y\"],mnet_inputs['labels']) #tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = anet_y, labels = mnet_inputs['labels']))\n",
    "\n",
    "# Operations for assessing the accuracy of the classifier\n",
    "#anet_accuracy_operation, _ = tf.contrib.metrics.cohen_kappa(tf.transpose(mnet_inputs['labels']), tf.transpose(anet_endpoint[\"y\"]), 5, class_weights)\n",
    "anet_accuracy_operation, anet_accuracy_op = tf.contrib.metrics.cohen_kappa(tf.argmax(mnet_inputs['labels'],1), tf.argmax(anet_endpoint[\"y\"],1), 5)\n",
    "\n",
    "confusion_anet = tf.confusion_matrix(labels=tf.argmax(mnet_inputs['labels'],1), predictions=tf.argmax(anet_endpoint[\"y\"],1), num_classes=5)\n",
    "\n",
    "vars = tf.trainable_variables()\n",
    "Anet1_vars = [v for v in vars if v.name.startswith(\"Anet1\")]\n",
    "Anet_vars = [v for v in vars if v.name.startswith(\"Anet2\")] + Anet1_vars\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) \n",
    "update_ops = [ops for ops in update_ops if not ops.name.startswith(\"InceptionResnetV2\")]\n",
    "with tf.control_dependencies(update_ops): # Ensures that we execute the update_ops before performing the train_step train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "    anet_train_step = tf.train.AdamOptimizer(config['learningrate']).minimize(anet_cross_entropy, var_list=Anet_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anet_fetches = {\n",
    "    'optimizer': anet_train_step,\n",
    "    'loss': anet_cross_entropy,\n",
    "    'auc': anet_accuracy_op,\n",
    "    'anet_endpoint':anet_endpoint   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anet_val_fetches = {\n",
    "    'loss': anet_cross_entropy,\n",
    "    'auc': anet_accuracy_op,\n",
    "    'anet_endpoint':anet_endpoint,\n",
    "    'confusion_matrix':confusion_anet\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeCurves(mnet_curves, handle=None):\n",
    "    if not handle:\n",
    "        handle = plt.figure(figsize = (15,5))\n",
    "\n",
    "    fig = plt.figure(handle.number, figsize = (15,5)) \n",
    "    fig.clear()\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    plt.cla()\n",
    "\n",
    "    counter = len(mnet_curves[list(mnet_curves.keys())[0]])\n",
    "    x = np.linspace(0, counter, num=counter)\n",
    "#     for key, value in mnet_curves.items():\n",
    "    value_ = np.array(mnet_curves['training_loss']).astype(np.double)\n",
    "    mask = np.isfinite(value_)\n",
    "#         plt.plot(x[mask], value_[mask], label=key)\n",
    "    ax.plot(x[mask], value_[mask], label=\"Training\")\n",
    "    value_ = np.array(mnet_curves['validation_loss']).astype(np.double)\n",
    "    mask = np.isfinite(value_)\n",
    "#         plt.plot(x[mask], value_[mask], label=key)\n",
    "    ax.plot(x[mask], value_[mask], label=\"Validation\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(\"A-Net losses\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    display.clear_output(wait=True)\n",
    "   \n",
    "#     plt.subplot(1, 2, 2)\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "#     ax2 = plt.axes()\n",
    "    plt.cla()\n",
    "\n",
    "#     counter = len(anet_curves[list(anet_curves.keys())[0]])\n",
    "    x = np.linspace(0, counter, num=counter)\n",
    "#     for key, value in anet_curves.items():\n",
    "    value_ = np.array(mnet_curves['training_accuracy']).astype(np.double)\n",
    "    mask = np.isfinite(value_)\n",
    "#         plt.plot(x[mask], value_[mask], label=key)\n",
    "    ax2.plot(x[mask], value_[mask], label=\"Training\")\n",
    "    value_ = np.array(mnet_curves['validation_accuracy']).astype(np.double)\n",
    "    mask = np.isfinite(value_)\n",
    "#         plt.plot(x[mask], value_[mask], label=key)\n",
    "    ax2.plot(x[mask], value_[mask], label=\"Validation\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(\"A-Net accuracy (Cohen's kappa)\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    display.clear_output(wait=True)\n",
    "   \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow Session and initialize all weights\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "tf.local_variables_initializer().run()\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run!\n",
    "'''\n",
    "0 - No DR\n",
    "1 - Mild\n",
    "2 - Moderate\n",
    "3 - Severe\n",
    "4 - Proliferative DR\n",
    "'''\n",
    "graph = tf.Graph()\n",
    "mnet_saver = tf.train.import_meta_graph('/content/gdrive/My Drive/mnet_models/1.ckpt.meta')\n",
    "mnet_saver.restore(sess, tf.train.latest_checkpoint('/content/gdrive/My Drive/mnet_models/'))\n",
    "# Get the placeholders from the graph by name\n",
    "mnet_data = tf.get_default_graph().get_tensor_by_name(\"mnet_data_left:0\")\n",
    "mnet_labels = tf.get_default_graph().get_tensor_by_name(\"mnet_labels_training:0\")\n",
    "# Tensors we want to evaluate\n",
    "MNet_feature_map = tf.get_default_graph().get_tensor_by_name(\"InceptionResnetV2/InceptionResnetV2/Conv2d_7b_1x1/Relu:0\")\n",
    "\n",
    "colors = [(1, 0, 0), (0, 0, 1)]  # R -> G -> B\n",
    "cmap_name = 'my_list'\n",
    "cm = LinearSegmentedColormap.from_list(cmap_name, colors)\n",
    "#CHECK_PATH\n",
    "df_long_labels = pd.read_csv(\"/content/gdrive/My Drive/trainLabels.csv\")\n",
    "for e in range(config['numEpochs']):\n",
    "    print(\"Epoch:\",e)\n",
    "    timestamp1 = time.time()\n",
    "    training_labels, augmented_list,file_names = augment_image(\"train\", 1, df_long_labels)\n",
    "    numTrainSamples = len(augmented_list)\n",
    "    augemnted_array = np.array(augmented_list)\n",
    "    mnet_avg_loss_in_current_epoch = 0\n",
    "    mnet_avg_auc_in_current_epoch = 0\n",
    "    anet_avg_loss_in_current_epoch = 0\n",
    "    anet_avg_auc_in_current_epoch = 0\n",
    "    for i in range(0, numTrainSamples, config['batchsize']):     \n",
    "        mnet_results = sess.run(MNet_feature_map, feed_dict={mnet_data: augemnted_array[i:i+config['batchsize']], mnet_labels: training_labels[i:i+config['batchsize']]})      \n",
    "        anet_results = sess.run(anet_fetches, feed_dict={mnet_inputs['anet']: mnet_results, mnet_inputs['labels']: training_labels[i:i+int(config['batchsize'])]})\n",
    "        anet_res = anet_results[\"anet_endpoint\"]\n",
    "        score_map = anet_res['S']\n",
    "        attn_map = anet_res['A']\n",
    "        g_map = anet_res['G']\n",
    "        val = i\n",
    "        anet_avg_loss_in_current_epoch += anet_results['loss']\n",
    "        anet_avg_auc_in_current_epoch += np.mean(anet_results['auc'])            \n",
    "        '''\n",
    "        for fig in range(config['batchsize']):\n",
    "          if e==config['numEpochs']-1:\n",
    "            gated_maps[file_names[val]] = g_map[fig,:,:,:]\n",
    "          display.clear_output(wait=True)\n",
    "          print(\"label:\",training_labels[val],file_names[val])\n",
    "          f = plt.figure(figsize=(20, 20))\n",
    "          a = f.add_subplot(1, 6, 1)\n",
    "          imgplot = plt.imshow(augemnted_array[val,:,:,:], cmap=\"hot\")\n",
    "          a.set_title('Training Image')\n",
    "          a = f.add_subplot(1,6,2)\n",
    "          a.set_title('Score map')\n",
    "          im=plt.imshow(score_map[fig,:,:,0], interpolation='nearest', origin='lower',cmap=cm)\n",
    "          #plt.colorbar(im)\n",
    "          #fig.colorbar(im, ax=a)\n",
    "          plt.subplot(163)\n",
    "          plt.imshow(score_map[fig,:,:,1],  interpolation='nearest', origin='lower',cmap=cm)\n",
    "          plt.subplot(164)\n",
    "          im1 = plt.imshow(score_map[fig,:,:,2],  interpolation='nearest', origin='lower', cmap=cm)\n",
    "          #plt.colorbar(im1)\n",
    "          plt.subplot(165)\n",
    "          plt.imshow(score_map[fig,:,:,3],  interpolation='nearest', origin='lower', cmap=cm)\n",
    "          plt.subplot(166)\n",
    "          plt.imshow(score_map[fig,:,:,4],  interpolation='nearest', origin='lower', cmap=cm)\n",
    "          plt.show()\n",
    "          f = plt.figure(figsize=(20, 20))\n",
    "          a = f.add_subplot(1,5,1)\n",
    "          a.set_title('Attention map')\n",
    "          a_im = plt.imshow(attn_map[fig,:,:,0],  interpolation='nearest', origin='lower',cmap=cm)\n",
    "          plt.subplot(152)\n",
    "          plt.imshow(attn_map[fig,:,:,1],  interpolation='nearest', origin='lower',cmap=cm)\n",
    "          plt.subplot(1,5,3)\n",
    "          a_im1 = plt.imshow(attn_map[fig,:,:,2],  interpolation='nearest', origin='lower', cmap=cm)\n",
    "          plt.subplot(1,5,4)\n",
    "          plt.imshow(attn_map[fig,:,:,3],  interpolation='nearest', origin='lower', cmap=cm)\n",
    "          plt.subplot(1,5,5)\n",
    "          plt.imshow(attn_map[fig,:,:,4],  interpolation='nearest', origin='lower', cmap=cm)\n",
    "          plt.show()\n",
    "          val+=1\n",
    "          '''\n",
    "    \n",
    "    # computing the average by deviding by the batch size\n",
    "    anet_avg_loss_in_current_epoch = anet_avg_loss_in_current_epoch /numTrainSamples\n",
    "    anet_avg_auc_in_current_epoch = anet_avg_auc_in_current_epoch /numTrainSamples\n",
    "    anet_curves['training_loss'] += [anet_avg_loss_in_current_epoch]\n",
    "    anet_curves['training_accuracy'] += [anet_avg_auc_in_current_epoch]\n",
    "    \n",
    "    validation_labels, validation_list,file_names = augment_image(\"Validation\", 1, df_long_labels)\n",
    "    numValSamples = len(validation_list)\n",
    "    validation_array = np.array(validation_list)\n",
    "    for i in range(0, numValSamples, config['batchsize']):\n",
    "        mnet_results = sess.run(MNet_feature_map, feed_dict={mnet_data: validation_array[i:i+config['batchsize']], mnet_labels: validation_labels[i:i+config['batchsize']]})\n",
    "        anet_results = sess.run(anet_val_fetches, feed_dict={mnet_inputs['anet']: mnet_results, mnet_inputs['labels']: validation_labels[i:i+int(config['batchsize'])]})\n",
    "        anet_res = anet_results[\"anet_endpoint\"]\n",
    "        score_map = anet_res['S']\n",
    "        attn_map = anet_res['A']\n",
    "        g_map = anet_res['G']\n",
    "        val = i\n",
    "        '''\n",
    "        for fig in range(config['batchsize']):\n",
    "            if e==config['numEpochs']-1:\n",
    "              gated_maps_val[file_names[val]] = g_map[fig,:,:,:]\n",
    "            display.clear_output(wait=True)\n",
    "            print(\"Validation label:\",validation_labels[val])\n",
    "            f = plt.figure(figsize=(20, 20))\n",
    "            a = f.add_subplot(1, 6, 1)\n",
    "            imgplot = plt.imshow(validation_array[val,:,:,:], cmap=\"hot\")\n",
    "            a.set_title('Training Image')\n",
    "            a = f.add_subplot(1,6,2)\n",
    "            a.set_title('Score map')\n",
    "            im=plt.imshow(score_map[fig,:,:,0], interpolation='nearest', origin='lower',cmap=cm)\n",
    "            #plt.colorbar(im)\n",
    "            #fig.colorbar(im, ax=a)\n",
    "            plt.subplot(163)\n",
    "            plt.imshow(score_map[fig,:,:,1],  interpolation='nearest', origin='lower',cmap=cm)\n",
    "            plt.subplot(164)\n",
    "            im1 = plt.imshow(score_map[fig,:,:,2],  interpolation='nearest', origin='lower', cmap=cm)\n",
    "            #plt.colorbar(im1)\n",
    "            plt.subplot(165)\n",
    "            plt.imshow(score_map[fig,:,:,3],  interpolation='nearest', origin='lower', cmap=cm)\n",
    "            plt.subplot(166)\n",
    "            plt.imshow(score_map[fig,:,:,4],  interpolation='nearest', origin='lower', cmap=cm)\n",
    "            plt.show()\n",
    "            f = plt.figure(figsize=(20, 20))\n",
    "            a = f.add_subplot(1,5,1)\n",
    "            a.set_title('Attention map')\n",
    "            a_im = plt.imshow(attn_map[fig,:,:,0],  interpolation='nearest', origin='lower',cmap=cm)\n",
    "            plt.subplot(152)\n",
    "            plt.imshow(attn_map[fig,:,:,1],  interpolation='nearest', origin='lower',cmap=cm)\n",
    "            plt.subplot(1,5,3)\n",
    "            a_im1 = plt.imshow(attn_map[fig,:,:,2],  interpolation='nearest', origin='lower', cmap=cm)\n",
    "            plt.subplot(1,5,4)\n",
    "            plt.imshow(attn_map[fig,:,:,3],  interpolation='nearest', origin='lower', cmap=cm)\n",
    "            plt.subplot(1,5,5)\n",
    "            plt.imshow(attn_map[fig,:,:,4],  interpolation='nearest', origin='lower', cmap=cm)\n",
    "            plt.show()\n",
    "            val+=1\n",
    "        '''\n",
    "        anet_avg_loss_in_current_epoch += anet_results['loss']\n",
    "        anet_avg_auc_in_current_epoch += np.mean(anet_results['auc'])\n",
    "    \n",
    "    anet_avg_loss_in_current_epoch = anet_avg_loss_in_current_epoch/numValSamples\n",
    "    anet_avg_auc_in_current_epoch = anet_avg_auc_in_current_epoch/numValSamples\n",
    "    anet_curves['validation_loss'] += [anet_avg_loss_in_current_epoch]\n",
    "    anet_curves['validation_accuracy'] += [anet_avg_auc_in_current_epoch]\n",
    "    \n",
    "    print('Done with epoch %d' % (e))\n",
    "    timestamp2 = time.time()    \n",
    "    print(\"End of Epoch %d took %.2f seconds\" % (e,timestamp2 - timestamp1))\n",
    "    visualizeCurves(anet_curves)\n",
    "    print('Anet Loss :',anet_avg_loss_in_current_epoch)\n",
    "    print('Anet Accuracy :',anet_avg_auc_in_current_epoch)\n",
    "    print('Anet Confusion Matrix:',anet_results['confusion_matrix'])\n",
    "    #print(gated_maps)\n",
    "    #CHECK_PATH ## create a new folder to save the models and \n",
    "    if (e%(config['numEpochs']-1))==0:\n",
    "        save_path = saver.save(sess, \"/content/gdrive/My Drive/anet_models/model\"+str(e)+\".ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
